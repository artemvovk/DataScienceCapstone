words <- unlist(strsplit(words[[1]]$content, " "))
words
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput("dank MEMES break's my HEAR71414 ()_*_AD)(^")
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput("dank MEMES break's my HEAR71414 ()_*_AD)(^")
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput("dank MEMES break's my HEAR71414 ()_*_AD)(^")
words
words[-1]
words[-length(words)]
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput("dank MEMES break's my HEAR71414 ()_*_AD)(^")
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput("dank MEMES break's my HEAR71414 ()_*_AD)(^")
corpus <- newsCorp
words <- tokenizeInput("HE SAID in")
words
tokenized <- nTokenize(x = corpus, n = length(words))
grams <- tokensDF(tokenized, gramSize = (length(words)+1))[first_terms == words$first_terms]
head(grams)
grams
grams <- tokensDF(tokenized, gramSize = (length(words)+1))
grams
length(words)
length(words)+1
tokenized <- nTokenize(x = corpus, n = length(words)+1)
grams <- tokensDF(tokenized, gramSize = (length(words)+1))
grams
grams[first_terms == words$first_terms]
grams <- calcProbs(grams)
grmas
grams
head(grams)
grams <- tokensDF(tokenized, gramSize = maxGram)[first_terms == words$first_terms]
grams
probGrams <- calcProbs(grams)
probGrams
head(probGrams)
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
probGrams <- calcProbs(grams)
head(probGrams)
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
probGrams <- calcProbs(grams)
probGrams[discount != 0]
probGrams[discount != 1]
head(probGrams)
probGrams[last_word == "that"]
grams[last_word == "that"]
probGrams[last_word == "i"]
grams[last_word == "i"]
probGrams[discount > 0]
probGrams[discount == 0]
probGrams[discount > 1]
probGrams[discount == 1]
probGrams
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
grams <- tokensDF(tokenized, gramSize = maxGram)[first_terms == words$first_terms]
grams
probGrams <- calcProbs(grams)
probGrams
lfProbGrams <- leftOverProbs(probGrams)
lfProbGrams
words
input
test <- c(1,2,3,4,5,6,7,8,9)
test
head(test, -2)
head(test, -3)
head(test, -5)
head(test, -1)
head(test, -0)
head(test)
head(test, -0)
input
words <- cleanInput(input)
words <- tokenizeInput(words)
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
words <- cleanInput(input)
words <- tokenizeInput(words)
words
input
length(input)
words <- cleanInput(input)
maxGram = length(words)+1
maxGram
words <- cleanInput(input)
maxGram = length(words)+1
if(maxGram < 2){
return(NULL)
}
words <- tokenizeInput(words, maxGram)
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
words
words <- cleanInput(input)
maxGram <- length(words)+1
words
words <- tokenizeInput(words, maxGram)
words
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
words <- cleanInput(input)
maxGram <- length(words)+1
words <- tokenizeInput(words, maxGram)
words
input <- "he said in"
words <- cleanInput(input)
maxGram <- length(words)+1
if(maxGram < 2){
return(NULL)
}
words <- tokenizeInput(words, maxGram)
words
tokenized <- nTokenize(x = corpus, n = maxGram)
grams <- tokensDF(tokenized, gramSize = maxGram)[first_terms == words$last_gram]
grams
probGrams <- calcProbs(grams)
probGrams
for(scope in length(words):2){
end_gram <- paste0(tail(words, scope-1), collapse = "_")
tokenized <- nTokenize(x = corpus, n = scope)
next_grams <- tokensDF(tokenized, gramSize = scope)[first_terms == end_gram]
next_grams <- calcProbs(next_grams)
grams <- rbind(grams, next_grams)
}
words <- cleanInput(input)
maxGram <- length(words)+1
if(maxGram < 2){
return(NULL)
}
words <- tokenizeInput(words, maxGram)
tokenized <- nTokenize(x = corpus, n = maxGram)
grams <- tokensDF(tokenized, gramSize = maxGram)[first_terms == words$last_gram]
if(nrow(grams) > 0){
grams <- calcProbs(grams)
}
for(scope in (maxGram-1):1){
words <- tokenizeInput(words, scope)
tokenized <- nTokenize(x = corpus, n = scope)
next_grams <- tokensDF(tokenized, gramSize = scope)[first_terms == words$last_gram]
next_grams <- calcProbs(next_grams)
grams <- rbind(grams, next_grams)
}
grams
corpus
corpus <- newsCorp
words <- cleanInput(input)
maxGram <- length(words)+1
if(maxGram < 2){
return(NULL)
}
words <- tokenizeInput(words, maxGram)
tokenized <- nTokenize(x = corpus, n = maxGram)
grams <- tokensDF(tokenized, gramSize = maxGram)[first_terms == words$last_gram]
if(nrow(grams) > 0){
grams <- calcProbs(grams)
}
for(scope in (maxGram-1):1){
words <- tokenizeInput(words, scope)
tokenized <- nTokenize(x = corpus, n = scope)
next_grams <- tokensDF(tokenized, gramSize = scope)[first_terms == words$last_gram]
next_grams <- calcProbs(next_grams)
grams <- rbind(grams, next_grams)
}
grams
maxGram
scope in (maxGram-1):1
for(scope in (maxGram-1):1){print(scope)}
words <- cleanInput(input)
maxGram <- length(words)+1
for(scope in (maxGram-1):1){print(scope)}
words <- tokenizeInput(words, maxGram)
tokenized <- nTokenize(x = corpus, n = maxGram)
grams <- tokensDF(tokenized, gramSize = maxGram)[first_terms == words$last_gram]
grams
for(scope in (maxGram-1):1){
words <- tokenizeInput(words, scope)
print(words)
}
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
for(scope in (maxGram-1):1){
words <- tokenizeInput(words, scope)
print(words)
}
words <- cleanInput(input)
maxGram <- length(words)+1
words <- tokenizeInput(words, maxGram)
words
input
words <- cleanInput(input)
maxGram <- length(words)+1
if(maxGram < 2){
return(NULL)
}
words <- tokenizeInput(words, maxGram)
words
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput(input)
tokenizeInput(cleanInput(input))
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput(cleanInput(input))
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput(cleanInput(input))
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput(cleanInput(input))
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput(cleanInput(input))
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput(cleanInput(input))
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput(cleanInput(input))
tail(words, 2)
tail(input, 2)
tail(input, 3)
tail(input, 1)
tail(input[[1]], 1)
tail(input[1], 1)
words <- unlist(strsplit(input[[1]]$content, " "))
words <- unlist(strsplit(input, " "))
input
words
tail(words,1)
tail(words,2)
tail(words,3)
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput(cleanInput(input))
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput(cleanInput(input))
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
tokenizeInput(cleanInput(input))
words <- tokenizeInput(cleanInput(input))
words[-1]
words[length(words)]
words
words[length(words),]
words[nrow(words),]
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
words <- cleanInput(input)
maxGram <- length(words)+1
if(maxGram < 2){
return(NULL)
}
words <- tokenizeInput(words)
tokenized <- nTokenize(x = corpus, n = maxGram)
grams <- tokensDF(tokenized, gramSize = maxGram)[first_terms == words[nrow(words),]$last_gram]
grams
tokenized
maxGram
input
words <- cleanInput(input)
words
lenght(words)
length(words)
length(words[[1]]$content)
words[[1]]$content
words <- cleanInput(input)
words <- tokenizeInput(words)
maxGram <- nrow(words)
if(maxGram < 2){
return(NULL)
}
maxGram
words <- cleanInput(input)
words <- tokenizeInput(words)
maxGram <- nrow(words)
if(maxGram < 2){
return(NULL)
}
tokenized <- nTokenize(x = corpus, n = maxGram+1)
grams <- tokensDF(tokenized, gramSize = maxGram+1)[first_terms == words[nrow(words),]$last_gram]
if(nrow(grams) > 0){
grams <- calcProbs(grams)
}
grams
for(scope in (maxGram):1){
tokenized <- nTokenize(x = corpus, n = scope)
next_grams <- tokensDF(tokenized, gramSize = scope)[first_terms == words[scope,]$last_gram]
next_grams <- calcProbs(next_grams)
grams <- rbind(grams, next_grams)
}
return(grams)
grams
scope
tokenized <- nTokenize(x = corpus, n = scope)
next_grams <- tokensDF(tokenized, gramSize = scope)[first_terms == words[scope,]$last_gram]
next_grams <- calcProbs(next_grams)
grams <- rbind(grams, next_grams)
grams
next_grams
words[scope,]
words[scope,]$last_gram
words[scope,]$last_gram[1]
words[scope,]$last_gram[[1]]
?data.frame
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
words <- cleanInput(input)
words <- tokenizeInput(words)
maxGram <- nrow(words)
if(maxGram < 2){
return(NULL)
}
tokenized <- nTokenize(x = corpus, n = scope)
next_grams <- tokensDF(tokenized, gramSize = scope)[first_terms == words[scope,]$last_gram]
next_grams <- calcProbs(next_grams)
grams <- rbind(grams, next_grams)
next_grams
grams
next_grams <- tokensDF(tokenized, gramSize = scope)
corpus
corpus[[1]]
corpus[[1]]$content
corpus[[100]]$content
next_grams
next_grams[first_terms == words[3,]]
next_grams[first_terms == words[3,]$last_gram]
next_grams[first_terms == words[2,]$last_gram]
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
words <- cleanInput(input)
words <- tokenizeInput(words)
maxGram <- nrow(words)
if(maxGram < 2){
return(NULL)
}
tokenized <- nTokenize(x = corpus, n = maxGram+1)
grams <- tokensDF(tokenized, gramSize = maxGram+1)[first_terms == words[nrow(words),]$last_gram]
if(nrow(grams) > 0){
grams <- calcProbs(grams)
}
for(scope in (maxGram):1){
tokenized <- nTokenize(x = corpus, n = scope)
next_grams <- tokensDF(tokenized, gramSize = scope)[first_terms == words[scope-1,]$last_gram]
next_grams <- calcProbs(next_grams)
grams <- rbind(grams, next_grams)
}
input <- "WHAT he SAID333 in"
words <- cleanInput(input)
words <- tokenizeInput(words)
maxGram <- nrow(words)
if(maxGram < 2){
return(NULL)
}
tokenized <- nTokenize(x = corpus, n = maxGram+1)
grams <- tokensDF(tokenized, gramSize = maxGram+1)[first_terms == words[nrow(words),]$last_gram]
if(nrow(grams) > 0){
grams <- calcProbs(grams)
}
for(scope in (maxGram):2){
tokenized <- nTokenize(x = corpus, n = scope)
next_grams <- tokensDF(tokenized, gramSize = scope)[first_terms == words[scope-1,]$last_gram]
next_grams <- calcProbs(next_grams)
grams <- rbind(grams, next_grams)
}
grams
next_grams
input
words <- cleanInput(input)
words
words <- tokenizeInput(words)
words
nrow(words)
tokenized <- nTokenize(x = corpus, n = maxGram+1)
grams <- tokensDF(tokenized, gramSize = maxGram+1)[first_terms == words[maxGram,]$last_gram]
grams
calcProbs(grams)
source('~/Dropbox/Coding/Coursera/Capstone/TextProcessor/model.R')
words <- cleanInput(input)
words <- tokenizeInput(words)
maxGram <- nrow(words)
if(maxGram < 2){
return(NULL)
}
tokenized <- nTokenize(x = corpus, n = maxGram+1)
grams <- tokensDF(tokenized, gramSize = maxGram+1)[first_terms == words[maxGram,]$last_gram]
grams
calcProbs(grams)
for(scope in (maxGram):2){
tokenized <- nTokenize(x = corpus, n = scope)
next_grams <- tokensDF(tokenized, gramSize = scope)[first_terms == words[scope-1,]$last_gram]
next_grams <- calcProbs(next_grams)
grams <- rbind(grams, next_grams)
}
grams
next_grams
grams
grams <- calcProbs(grams)
for(scope in (maxGram):2){
tokenized <- nTokenize(x = corpus, n = scope)
next_grams <- tokensDF(tokenized, gramSize = scope)[first_terms == words[scope-1,]$last_gram]
next_grams <- calcProbs(next_grams)
grams <- rbind(grams, next_grams)
}
grams
head(grams, 50)
all_freq <- grams[, sum(count), by= last_word]
all_freq
all_freq <- grams[, all_freq=sum(count), by=last_word]
all_freq <- grams[, freq=sum(count), by=last_word]
grams
all_freq <- grams[, freq=sum(count), by=last_word]
all_freq <- grams[, sum(count), by= last_word]
all_freq
all_freq <- grams[, freq:=sum(count), by= last_word]
all_freq
all_freq
all_freq <- grams[, sum(count), by= last_word]
all_freq
all_freq <- grams[,  sum(count*discount), by= last_word]
all_freq
grams[last_word == 'hand']
all_freq
colnames(all_freq)
colnames(all_freq) <- c("word", "frequency")
all_freq
freq_sum <- sum(grams$count)
all_freq <- grams[, sum(count*discount)/freq_sum, by= last_word]
colnames(all_Freq) <- c("word", "frequency")
freq_sum <- sum(grams$count)
all_freq <- grams[, sum(count*discount)/freq_sum, by= last_word]
colnames(all_freq) <- c("word", "frequency")
all_freq
freq_sum <- sum(grams$count)
all_freq <- grams[, sum(count*discount)/freq_sum, by= last_word]
colnames(all_freq) <- c("word", "prob")
all_freq
all_freq[word == "a"]
all_freq[word == "the"]
grams[last_wowrd == "the"]
grams[last_word == "the"]
all_freq[prob > 0.01])
all_freq[prob > 0.01]
runApp('Capstone/TextProcessor')
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
library(RJDBC)
install.packages("jsonlite")
install.packages("jsonlite")
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
READDEVcon <- dbConnect(drv, "jdbc:oracle:thin:@vacols.dev.vaco.va.gov:1526:BVAP", "DSUSERRO", "June.2017")
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
?JDBC
?JDBC
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
Sys.getenv("VACOLS_DEV_USER")
R.home()
profile()
help("Rprofile")
VACOLS_DEV_USER
Sys.getenv("VACOLS_DEV_USER")
?Startup
Sys.getenv("R_HOME")
Sys.getenv("R_HOME")
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
Sys.getenv("VACOLS_DEV_USER")
READDEVcon <- dbConnect(drv, "jdbc:oracle:thin:@vacols.dev.vaco.va.gov:1526:BVAP", "DSUSERRO", "June.2017")
PRODcon <- dbConnect(drv, "jdbc:oracle:thin:@vacols.vaco.va.gov:1526:BVAP", Sys.getenv("VACOLS_PROD_USER"), Sys.getenv("VACOLS_PROD_PASSWORD"))
Sys.getenv("VACOLS_PROD_USER")
Sys.getenv("VACOLS_PROD_PASSWORD")
Sys.getenv("VACOLS_DEV_PASSWORD")
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
VACOLSfetch(DEVcon, "select * from DECASS where ROWNUM <= 100")
VACOLSfetch(DEVcon, "select * from DECASS where ", "ROWNUM <= 100")
VACOLSfetch(DEVcon, getCaseAssignments, "ROWNUM <= 100")
VACOLSfetch(PRODcon, getCaseAssignments, "ROWNUM <= 100")
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
VACOLSfetch(DEVcon, getAODappeal, "BFKEY='888111'")
VACOLSfetch(DEVcon, getAODappeal, "ROWNUM <= 100")
VACOLSfetch(DEVcon, getAODappeal, "ROWNUM <= 1000")
VACOLSfetch(DEVcon, getAODappeal, "AOD > 0")
VACOLSfetch(DEVcon, getAODappeal, "AOD_DIARIES > 0")
VACOLSfetch(DEVcon, getAODappeal, "BFKEY='888111'")
VACOLSfetch(DEVcon, getAODappeal, "AOD_DIARIES.CNT > 0")
VACOLSfetch(DEVcon, getAODappeal, "ROWNUM < 10")
source('~/Dropbox/Work/Caseflow/vacolsQuery.R')
VACOLSfetch(DEVcon, getAODappeal, "BRIEFF.BFKEY='888111'")
VACOLSfetch(DEVcon, getAODappeal, "ROWNUM < 100")
VACOLSfetch(DEVcon, getAODappeal, "BRIEFF.BFKEY='1995472'")
VACOLSfetch(DEVcon, getAODappeal, "BRIEFF.BFKEY='1995473'")
VACOLSfetch(DEVcon, getAODappeal, "BRIEFF.BFKEY='1995472'")
VACOLSfetch(DEVcon, getAODappeal, "BRIEFF.BFKEY='1995470'")
VACOLSfetch(DEVcon, getAODappeal, "BRIEFF.BFKEY='1995'")
source('~/Dropbox/Work/Caseflow/vacolsshit.R')
source('~/Dropbox/Work/Caseflow/vacolsshit.R')
source('~/Dropbox/Work/Caseflow/vacolsshit.R')
VACOLSfetch(DEVcon, getIssues, "ISSCODE !='15' and ROWNUM <= 100")
VACOLSfetch(DEVcon, getIssues, "ISSCODE !='1' and ROWNUM <= 100")
VACOLSfetch(DEVcon, getIssues, "ISSKEY ='494800'")
VACOLSfetch(DEVcon, getCaseAssignments, "BFKEY=  '494800'")
getCaseAssignments
dbFetch(dbSendQuery(DEVcon, "select * from DECASS join BRIEFF on BRIEFF.BFKEY=DECASS.DEFLODER where BFKEY='494800'"))
dbFetch(dbSendQuery(DEVcon, "select * from DECASS join BRIEFF on BRIEFF.BFKEY=DECASS.DEFOLDER where BFKEY='494800'"))
VACOLSfetch(DEVcon, getCaseAssignments, "ROWNUM <= 100")
VACOLSfetch(DEVcon, getCaseAssignments, "CSS_ID is not null and ROWNUM <= 100")
VACOLSfetch(DEVcon, getCaseAssignments, "STAFF.SDOMAINID is not null and ROWNUM <= 100")
source('~/Dropbox/Work/Caseflow/vacolsshit.R')
options(internet.info = 0)
options(internet.info = 1)
options(internet.info = 0)
file.edit('~/.config/RStudio/desktop.ini')
$R_HOME
dir()
setwd("Capstone/TextProcessor/")
shiny::runApp()
path <- paste("~/Downloads/SwiftKeyData/", lang, "/", lang, ".", source, ".txt", sep = "")
runApp()
runApp()
runApp()
